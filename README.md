# uber_highload

## 1. Целевая аудитория
- 91 млн. активных пользователей
- 4 млн. водителей-партнеров
- 14 млн. поездок ежедневно
- Основная аудитория сервиса находится в северной Америке и Европе

## 2. Расчет нагрузки

В городе Нью-Йорке, где очень много пользуются такси, в начале 2020 года, в день совершалось около 520к поездок, при 69к водителей на линии https://www.businessofapps.com/data/uber-statistics/. Возьмем эту статистику и рассчитаем, сколько примерно водителей на линии в мире одновременно:
**14 000к / 520к * 69к = 1 858к**

Водители будут каждые 3 секунды отсылать свои координаты.
1 858 000 / 3 = 619 334 rps.
Водитель посылает свой id (4 байта), широту (8 байт), долготу (8 байт).
Каждый 3 секунды, водители будут присылать 36Мб данных о своем местоположении. В день будет перадавься 1Тб данных с местоположением водителей.
Для экономии трафика водителей данные будем передавать по UDP, т.к. данные не критичны и мы можем их терять.

Расчитаем пиковую нагрузку, которую создают клиенты. 14 000 000 / 86 400 * 3 = 486 rps.
На запрос пользователя, будем выдавать максимум 5 ближайших машин.

|         | Кол-во RPS|
|---------| ----------|
|Водители | 619к      |
|Клиенты  | 500       |

## 3. Логическая схема базы данных
![Снимок экрана 2021-05-28 в 20 50 43](https://user-images.githubusercontent.com/43621139/120025096-d74eea80-bff8-11eb-9b4a-4790ac01f7f4.png)

## 4. Физическая схема базы данных
- для хранения информации о клиентах, водителях, поездках будем использовать Postgres
- для хранения геолокации водителей будем использовать Postgres вместе с расширением PostGIS
- для сессий водителей и клиентов возьмем Redis

Для базы данных с геопозицией будем использовать шардирование. Шардирование будет делаться на основании геопозиции водителя. Выделим в США и Европе города, в которых больше всего пользуются такси и сделаем для них отдельные шарды. Выбор в какой шард делать запрос будет делаться на уровне сервиса геолокации на основании геополокации клиента.

Жители Нью-Йорка совершают 1/27 от общего числа поездок в день в нашем сервисе. Разобьем нашу базу для хранения геоданных на 27 шардов. 9 Для США и 18 для Европы, т.к. население Европы почти в 2 раза больше чем население США.

Также для надеждность воспользуемся репликацией. Использовать будем схему мастер - реплика. На каждый шард будет отдельный мастер и 2 реплики. Читать будем из реплик, в случае, если мастер упал, одна из реплик становится мастером.

Для одного кортежа необходимо 44 байт. Для хранения геолокации всех 4 млн водителей нужно 168 Мб. Для 1 шарда необходимо 6.5 Мб + 13 Мб для 2-х реплик.
Итого: для мастер + 2 реплики необходимо 20 Мб, а всего необходимо 540 Мб.

![Снимок экрана 2021-05-28 в 20 50 43 2](https://user-images.githubusercontent.com/43621139/120025108-dd44cb80-bff8-11eb-8d7b-1558c7082104.png)

## 5. Выбор технологий
В качестве языка для backend разработки выберем Go. Это компилируемый, статически типизуермый язык. Он достаточно производительный и потребляет адекватное количество ресурсов. Он прост в изучении, что позволит нанимать программистов с опытом работы на других языках. Упор будем делать на мобильные приложения, поэтому для их разработки выберем языки Swift и Kotlin. Для организации очередей будем использовать kafka.

## 6. Схема проекта
![Снимок экрана 2021-05-28 в 17 21 22](https://user-images.githubusercontent.com/43621139/119998433-5d5b3900-bfd9-11eb-9850-6e38e8bfc6f1.png)

## 7. Выбор оборудования
1) Выбор оборудования для балансировщиков нагрузки
- Nginx'ы, которые будут работать в Европе должны выдерживать 400к+ rps от водителей. Исходя из тестов производительности https://www.nginx.com/blog/testing-the-performance-of-nginx-and-nginx-plus-web-servers/, Nginx с 16 ядрами и 16 Гб ОЗУ выдерживает нагрузку в 1.2кк rps, при условии, что размере запроса < 1 Кб. Чтобы увеличить надежность, поставим 2 машины.
- Балансировщики в США должны выдерживать нагрузку 200к+ rps. Для этого нам понадобится сервер с 8 ядрами с 16 Гб ОЗУ, такая конфигурация выдеживает 640к rps. Для надежности, установим 2 сервера.
2) 
