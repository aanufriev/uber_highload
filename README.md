# uber_highload

## 1. Целевая аудитория
- 91 млн. активных пользователей
- 4 млн. водителей-партнеров
- 14 млн. поездок ежедневно
- Основная аудитория сервиса находится в северной Америке и Европе

## 2. Расчет нагрузки

В городе Нью-Йорке, где очень много пользуются такси, в начале 2020 года, в день совершалось около 520к поездок, при 69к водителей на линии https://www.businessofapps.com/data/uber-statistics/. Возьмем эту статистику и рассчитаем, сколько примерно водителей на линии в мире одновременно:
**14 000к / 520к * 69к = 1 858к**

Водители будут каждые 3 секунды отсылать свои координаты.
1 858 000 / 3 = 619 334 rps.
Водитель посылает свой id (4 байта), широту (8 байт), долготу (8 байт).
Каждый 3 секунды, водители будут присылать 36Мб данных о своем местоположении. В день будет перадавься 1Тб данных с местоположением водителей.
Для экономии трафика водителей данные будем передавать по UDP, т.к. данные не критичны и мы можем их терять.

Расчитаем пиковую нагрузку, которую создают клиенты. 14 000 000 / 86 400 * 3 = 486 rps.
На запрос пользователя, будем выдавать максимум 5 ближайших машин.

|         | Кол-во RPS|
|---------| ----------|
|Водители | 619к      |
|Клиенты  | 500       |

## 3. Логическая схема базы данных
![Снимок экрана 2021-05-31 в 21 17 54](https://user-images.githubusercontent.com/43621139/120234369-79bcd700-c260-11eb-8dae-be9aea3bec68.png)

## 4. Физическая схема базы данных
- для хранения информации о клиентах, водителях, поездках будем использовать Postgres
- для хранения геолокации водителей будем использовать Postgres вместе с расширением PostGIS
- для сессий водителей и клиентов возьмем Redis

Для базы данных с геопозицией будем использовать шардирование. Шардирование будет делаться на основании геопозиции водителя. Выделим в США и Европе города, в которых больше всего пользуются такси и сделаем для них отдельные шарды. Выбор в какой шард делать запрос будет делаться на уровне сервиса геолокации на основании геополокации клиента.

Жители Нью-Йорка совершают 1/27 от общего числа поездок в день в нашем сервисе. Разобьем нашу базу для хранения геоданных на 30 шардов. 10 Для США и 20 для Европы, т.к. население Европы почти в 2 раза больше чем население США.

Также для надеждность воспользуемся репликацией. Использовать будем схему мастер - реплика. На каждый шард будет отдельный мастер и 2 реплики. Читать будем из реплик, в случае, если мастер упал, одна из реплик становится мастером.

Для одного кортежа необходимо 44 байт. Для хранения геолокации всех 4 млн водителей нужно 168 Мб. Для 1 шарда необходимо 6.5 Мб + 13 Мб для 2-х реплик.
Итого: для мастер + 2 реплики необходимо 20 Мб, а всего необходимо 540 Мб.

Базу для хранения информации о поездках также разобьем на 30 шардов, 10 для США и 20 для Европы.
Когда водитель будет исполнять заказ, будет сохранять маршрут всей поездки. Допустим, средняя поездка длится 15 минут, тогда для хранения маршрута одной поездки, нам понадобится 15 * 60 / 3 * (4 + 32 + 8) = 13 200 байт. Для хранения маршрутов за 1 день понадобится 172 Гб.

![Снимок экрана 2021-05-31 в 21 17 54 2](https://user-images.githubusercontent.com/43621139/120234380-7f1a2180-c260-11eb-9869-33f6468a6cbe.png)

## 5. Выбор технологий
В качестве языка для backend разработки выберем Go. Это компилируемый, статически типизуермый язык. Он достаточно производительный и потребляет адекватное количество ресурсов. Он прост в изучении, что позволит нанимать программистов с опытом работы на других языках. Упор будем делать на мобильные приложения, поэтому для их разработки выберем языки Swift и Kotlin. Для организации очередей будем использовать kafka.

## 6. Схема проекта
![Снимок экрана 2021-05-31 в 22 35 37](https://user-images.githubusercontent.com/43621139/120234401-8ccfa700-c260-11eb-874d-9dfc61923b60.png)

## 7. Выбор оборудования
1) Выбор оборудования для балансировщиков нагрузки
- Nginx'ы, которые будут работать в Европе должны выдерживать 400к+ rps от водителей. Исходя из тестов производительности https://www.nginx.com/blog/testing-the-performance-of-nginx-and-nginx-plus-web-servers/, Nginx с 16 ядрами и 16 Гб ОЗУ выдерживает нагрузку в 1.2кк rps, при условии, что размере запроса < 1 Кб. Чтобы увеличить надежность, поставим 2 машины.
- Балансировщики в США должны выдерживать нагрузку 200к+ rps. Для этого нам понадобится сервер с 8 ядрами с 16 Гб ОЗУ, такая конфигурация выдеживает 640к rps. Для надежности, установим 2 сервера.

| CPU, Ядра |  RAM, Гб   |  SSD, Гб  |число машин|
|-----------| -----------|-----------|-----------|
| 16        | 16         |           | 4         |

2) Для kafka достаточно будет по 3 сервера на каждом континенте https://engineering.linkedin.com/kafka/benchmarking-apache-kafka-2-million-writes-second-three-cheap-machines

| CPU, Ядра |  RAM, Гб   |  SSD, Гб  |число машин|
|-----------| -----------|-----------|-----------|
| 6         | 32         | 50        | 6         |

3) Сервис геолокаций

| CPU, Ядра |  RAM, Гб   |  SSD, Гб  |число машин|
|-----------| -----------|-----------|-----------|
| 32        | 32         | 50        | 30        |

4) Сервис для работы с заказами

| CPU, Ядра |  RAM, Гб   |  SSD, Гб  |число машин|
|-----------| -----------|-----------|-----------|
| 32        | 32         | 50        | 30        |

5) Основной бекэнд

| CPU, Ядра |  RAM, Гб   |  SSD, Гб  |число машин|
|-----------| -----------|-----------|-----------|
| 32        | 32         | 50        | 4         |

6) Postgres с координатами водителей

| CPU, Ядра |  RAM, Гб   |  SSD, Гб  |число машин|
|-----------| -----------|-----------|-----------|
| 32        | 32         | 5         | 30        |

7) Postgres с заказами и маршрутами

| CPU, Ядра |  RAM, Гб   |  SSD, Гб  |число машин|
|-----------| -----------|-----------|-----------|
| 32        | 32         | 5000      | 30        |
